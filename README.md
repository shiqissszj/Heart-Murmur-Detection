# Heart Murmur Detection

This project builds deep learning models (primarily ResNet50) on heart sound (PCG) signals to detect cardiac murmurs. The pipeline includes stratified data splitting, spectrogram feature extraction, model training, and integrated evaluation.

## Key Features
- Stratified dataset split (train/vali/test) to maintain label distribution.
- Log-Mel spectrogram features are generated from WAV and chunked for CNN input.
- Two model backbones: ResNet50 and ResNet50+Dropout, supporting:
  - Multi-class: Murmur (Present/Unknown/Absent)
  - Binary: BinaryPresent (Present/Unknown vs Others), BinaryUnknown (Absent/Unknown vs Others)
- Automatically saves best models as TorchScript `.pt`.
- Placeholder interfaces for DBRes and XGBoost integration (implementations currently missing in the repo).

## Directory Structure
```
Heart-Murmur-Detection/
├─ main.py                         # Full pipeline entry (split + multi-task training + integration placeholder)
├─ train_resnet.py                 # Single-task training entry (CLI)
├─ stratified_data_split.py        # Stratified splitting → data/stratified_data/*
├─ DataProcessing/                 # Data and feature processing
│  ├─ find_and_load_patient_files.py
│  ├─ helper_code.py
│  ├─ label_extraction.py         # Extract labels from .txt (murmur/outcome/locations/pitch)
│  ├─ compute_LogMelSpecs.py      # Waveform → Log-Mel → chunking
│  ├─ mel_features.py             # STFT and Mel filters
│  ├─ net_feature_extractor.py    # Aggregate spectrograms and labels (see caveats)
│  └─ XGBoost_features/           # Hand-crafted features (for integration/analysis)
├─ HumBugDB/                      # Mainline models and training
│  ├─ runTorch.py                 # Binary classification training (TorchScript saving)
│  ├─ runTorchMultiClass.py       # Multi-class training (TorchScript saving)
│  ├─ ResNetSource.py             # ResNet backbones
│  ├─ ResNetDropoutSource.py      # ResNet variant with dropout
│  └─ LogMelSpecs/                # Alternative spectrogram implementation (different import path)
├─ runTorch.py                    # Top-level legacy/experimental training (.pth saving; avoid mixing)
├─ compute_LogMelSpecs.py         # Top-level counterpart (same functionality as DataProcessing)
├─ mel_features.py
├─ helper_code.py
├─ find_and_load_patient_files.py
├─ SQ.py / wav.py / plot.py       # Inference/demo scripts (non-mainline)
└─ models/                        # Training outputs (created at runtime)
```

## Training Entry Points and Flow

### 1) Full Pipeline: `main.py`
- Steps:
  1. Stratified split: calls `stratified_data_split.stratified_test_vali_split` to split `training_data/` into `data/stratified_data/{train,vali,test}/`.
  2. Train three tasks via `train_resnet.run_model_training`, saving to `models/`:
     - `Murmur` (multi-class → `models/model_Murmur.pt`)
     - `BinaryPresent` (binary → `models/model_BinaryPresent.pt`)
     - `BinaryUnknown` (binary → `models/model_BinaryUnknown.pt`)
  3. Optional DBRes/XGBoost integration: placeholders exist but required files are missing; implement or comment out accordingly.
- Example:
```
python main.py \
  --full_data_directory training_data \
  --stratified_directory data/stratified_data \
  --spectrogram_directory data/spectrograms \
  --model_name resnet50dropout
```

### 2) Single Task Entry: `train_resnet.py`
- Trains a single task (`--classes_name`) with configurable model and spectrogram cache directory.
- Saves the best model as TorchScript `.pt` named `models/model_<model_label>.pt`.
- Example:
```
python train_resnet.py \
  --classes_name murmur \
  --train_data_directory data/stratified_data/train_data \
  --vali_data_directory data/stratified_data/vali_data \
  --spectrogram_directory data/spectrograms \
  --model_name resnet50dropout \
  --model_label Murmur \
  --model_dir models
```
Available `classes_name`: `murmur | outcome | binary_present | binary_unknown`

## Data Preparation
- Raw data directory: `training_data/`
  - Follows the Challenge format: per-patient `.txt` (metadata + labels) and associated `.wav/.hea/.tsv` files.
- Stratified output directory: `data/stratified_data/{train_data, vali_data, test_data}` (generated by `main.py` or by running `stratified_data_split.py` directly).

## Feature Extraction (Log-Mel)
- Entry: `DataProcessing/compute_LogMelSpecs.py::waveform_to_examples`
  - Mono conversion → resample to `hyperparameters.SAMPLE_RATE`
  - Compute Log-Mel spectrogram
  - Chunk by window/hop, returning tensor `[N, 1, H, W]`
- Aggregator: `DataProcessing/net_feature_extractor.py`
  - Iterates multi-location recordings per patient and builds spectrogram chunks;
  - Aligns and expands `murmur/outcome/systolic/diastolic pitch` labels to match chunks;
  - Returns training/validation (or single-directory) `X, y`.

## Models and Training
- Mainline is under `HumBugDB/`:
  - `runTorch.py` (binary):
    - Models: `ResnetFull` / `ResnetDropoutFull` with `ResNet50` backbone and a 32-dim head + Sigmoid.
    - Loss: BCELoss; Optimizer: Adam; Metric: balanced accuracy; Saves: TorchScript `.pt`.
  - `runTorchMultiClass.py` (multi-class):
    - Models: `ResnetFull(n_classes)` / `ResnetDropoutFull(n_classes)` with CrossEntropyLoss;
    - Metric: macro precision from `classification_report`; Saves: TorchScript `.pt`.
  - Backbones: `HumBugDB/ResNetSource.py` (vanilla) and `HumBugDB/ResNetDropoutSource.py` (with dropout).
- During training, spectrograms `[N,1,H,W]` are expanded via `repeat(1,3,1,1)` to fit ResNet50’s 3-channel input.

## Outputs
- Models: `models/model_<Label>.pt` (TorchScript; loadable via `torch.jit.load`).
- Logs: Console outputs including loss, accuracy, and confusion/report (for multi-class).
- Optional spectrogram cache: saved to `data/spectrograms/` (e.g., `spec_train/spec_test/...`) if cache logic is enabled.

## Environment
- Python 3.8+
- PyTorch, NumPy, SciPy, scikit-learn, tqdm, resampy, librosa (librosa required for XGBoost features)
- GPU (optional): CUDA is used automatically if available
